# -*- coding: utf-8 -*-
"""PROJECT-1SDoH_training_evaluation_nsoley1.ipynb

Automatically generated by Colab.
# Loading the dataset
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install --user --upgrade scikit-learn gensim nltk transformers
import nltk
nltk.download('punkt')
import pandas as pd

import pandas as pd
notes_1=pd.read_csv('/content/drive/My Drive/Colab Notebooks/nlp project/NOTEEVENTS.csv')

patients=pd.read_csv('/content/drive/My Drive/Colab Notebooks/nlp project/PATIENTS.csv')

"""# Inclusion criteria"""

df_patients = patients[patients['GENDER'] == 'F']
female_notes = pd.merge(df_patients, notes_1, on='SUBJECT_ID',how='inner')
female_notes=female_notes[female_notes['CATEGORY']=='Discharge summary']
female_notes

icd_diag=pd.read_csv('/content/drive/My Drive/Colab Notebooks/nlp project/D_ICD_DIAGNOSES.csv')

pregnancy_related = icd_diag[
    icd_diag['LONG_TITLE'].str.contains('pregnancy', case=False) |
    icd_diag['ICD9_CODE'].str.startswith(tuple(['V22.0', 'V22.1', 'V22.2', 'V23.9', 'V24.2', '630', '631', '632', '633', '634', '635', '636', '637','640', '641', '642', '643', '644', '645', '646', '647', '648', '649']))
]

pregnancy_related_excluding_normal_delivery = pregnancy_related[
    ~pregnancy_related['ICD9_CODE'].str.startswith(('650', '651', '652', '653', '654', '655', '656', '657', '658', '659'))
]

pregnancy_related_normal_delivery = pregnancy_related[
    pregnancy_related['ICD9_CODE'].str.startswith(('650', '651', '652', '653', '654', '655', '656', '657', '658', '659'))
]

female_notes_not_normal = pd.merge(female_notes, pregnancy_related_excluding_normal_delivery, left_on='ROW_ID_x', right_on='ROW_ID', how='inner')
female_notes_not_normal = female_notes_not_normal.drop_duplicates(subset=['note_id'], keep='last') #keeps latest pregnacy notes in case of multiple
female_notes_normal=pd.merge(female_notes, pregnancy_related_normal_delivery, left_on='ROW_ID_x', right_on='ROW_ID', how='inner')
female_notes_normal = female_notes_normal.drop_duplicates(subset=['note_id'], keep='last') #keeps latest pregnacy notes in case of multiple

notes_not_normal_sample = female_notes_not_normal[['TEXT','ROW_ID','SUBJECT_ID','GENDER', 'DOB']]
notes_not_normal_sample['complication']=1
notes_normal_sample = female_notes_normal[['TEXT','ROW_ID','SUBJECT_ID','GENDER', 'DOB']]
notes_normal_sample['complication']=0

combined_notes = pd.concat([notes_not_normal_sample, notes_normal_sample])
combined_notes

combined_notes=combined_notes.reset_index()

"""# Upload manually annotated notes."""

mimic_annotations=pd.read_csv('/content/drive/My Drive/Colab Notebooks/nlp project/forannotation_frommimic.csv')

mimic_annotations= mimic_annotations.join(combined_notes, how='inner', lsuffix='_mimic', rsuffix='_combined')
mimic_annotations

mimic_annotations=mimic_annotations[['TEXT_mimic', 'ROW_ID_mimic',
       'SUBJECT_ID_mimic', 'Social Support', 'Occupation', 'Tobbaco',
       'Alcohol', 'Drug', 'TEXT_combined', 'complication']]

"""# Below are 182 notes for model development and evaluation with ground truth, let start by building a rule based model to extract SDOH

"""

notes_model=pd.read_csv('/content/drive/My Drive/Colab Notebooks/nlp project/notes_model.csv')

"""# Keyword Processor
# Social support
"""

!pip3 install --user nltk flashtext

sentence=notes_model['TEXT_combined'][0]
kp_ss = KeywordProcessor()
keyword_dict = {
    "kp_social_support": ["live with someone", "live with family", "live with friends",'lives with husband','lives with', 'supportive','support','married','husband','boyfriend','lives with'],
}
kp_ss.add_keywords_from_dict(keyword_dict)
print(kp_ss.extract_keywords(sentence))
print()

from IPython.core.display import display, HTML
display(HTML("<style>.container { width:90% !important; }</style>"))
from flashtext import KeywordProcessor
# from keyword_extractor import KeywordProcessor
from sklearn.base import BaseEstimator, ClassifierMixin
import numpy as np

class KeywordClassifier(ClassifierMixin, BaseEstimator):
    def __init__(self, keywords):
        self.keywords = keywords
        self.classes_ = ['No Social support',' Social Support']

    def fit(self, X, y, keywords=None):
        if keywords is not None:
            self.keywords = keywords
        return self

    def predict(self, X):
        output = []
        for document in X:
            extractions = self.keywords.extract_keywords(document)
            # Check if any keyword is extracted
            if len(extractions)>0:
                output.append(1)
            else:
                output.append(0)
        return np.array(output)

    def predict_proba(self, X):
        return self.predict(X)
    def predict_log_proba(self, X):
        proba = self.predict_proba(X)
        return np.log(proba)
classifier_ss = KeywordClassifier(kp_ss)

#First let's load data
import pandas as pd
from sklearn.model_selection import train_test_split
df=notes_model.copy()
df.loc[df['Social Support']==-1, 'Social Support']=0
X = df.TEXT_mimic.to_list()
y = df['Social Support'].to_list()

#here we are breaking the data into two sets 60:40, each with
X_train, X_test, y_train, y_test = train_test_split(X, y,stratify=y, test_size=0.4, shuffle=True, random_state=1234)

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
from sklearn.metrics import classification_report
y_pred_test = classifier_ss.predict(X_test)
cm = confusion_matrix(y_test, y_pred_test)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=classifier_ss.classes_)
disp.plot()
print(classification_report(y_test, y_pred_test, target_names=classifier_ss.classes_))

"""# Occupation"""

kp_occup = KeywordProcessor()
keyword_dict = {
    "kp_occupation": ["employed", "retired","work as",' worked in','works in', 'works as','worked','job','employement','quit',
                      'unemployed','jobless','working','works']
}
kp_occup.add_keywords_from_dict(keyword_dict)

from IPython.core.display import display, HTML
display(HTML("<style>.container { width:90% !important; }</style>"))
from flashtext import KeywordProcessor
# from keyword_extractor import KeywordProcessor
from sklearn.base import BaseEstimator, ClassifierMixin
import numpy as np

class KeywordClassifier(ClassifierMixin, BaseEstimator):
    def __init__(self, keywords):
        self.keywords = keywords
        self.classes_ = ['No Occupation','Occupation']

    def fit(self, X, y, keywords=None):
        if keywords is not None:
            self.keywords = keywords
        return self

    def predict(self, X):
        output = []
        for document in X:
            extractions = self.keywords.extract_keywords(document)
            # Check if any keyword is extracted
            if len(extractions)>0:
                output.append(1)
            else:
                output.append(0)
        return np.array(output)

    def predict_proba(self, X):
        return self.predict(X)
    def predict_log_proba(self, X):
        proba = self.predict_proba(X)
        return np.log(proba)
classifier_occup = KeywordClassifier(kp_occup)

import pandas as pd
from sklearn.model_selection import train_test_split
df=notes_model.copy()
df.loc[df['Occupation']==-1, 'Occupation']=0
#X is refered to as the input data, y the labeled data
X = df.TEXT_mimic.to_list()
y = df['Occupation'].to_list()
#here we are breaking the data into two sets of approximately 50%, each with
X_train, X_test, y_train, y_test = train_test_split(X, y,stratify=y, test_size=0.5, shuffle=True, random_state=1234)

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
from sklearn.metrics import classification_report
y_pred_test = classifier_occup.predict(X_test)
cm = confusion_matrix(y_test, y_pred_test)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=classifier_occup.classes_)
disp.plot()
print(classification_report(y_test, y_pred_test, target_names=classifier_occup.classes_))

"""# Substance use"""

# Create and initialize the KeywordProcessor with your keyword dictionary
kp_substance = KeywordProcessor()
keyword_dict = {
    "substanceuse": [
        "alcohol use", "drink alcohol", "consume alcohol", "alcohol consumption", "drink", "drinks", "drinking",
        "tobacco use", "smoke", "cigarette", "nicotine", "smoke", "smokes", "smoking",'pack','packs'
        "drug use", "substance abuse", "illicit drug use", "use drugs", "drug consumption", "drug habit", "drug addiction",
        "no alcohol use", "no drinking", "no alcohol consumption", "not drink alcohol", "not consume alcohol", "not drinking",
        "no tobacco use", "not smoke", "not smoking", "not cigarette", "no nicotine", "not smoking",
        "no drug use", "not use drugs", "no substance abuse", "not illicit drug use", "no drug consumption", "no drug habit", "no drug addiction"
    ]
}
kp_substance.add_keywords_from_dict(keyword_dict)

class SubstanceClassifier(ClassifierMixin, BaseEstimator):
    def __init__(self, keywords):
        self.keywords = keywords
        self.classes_ = ['No Substance Use', 'Substance Use']

    def fit(self, X, y, keywords=None):
        if keywords is not None:
            self.keywords = keywords
        return self
    def predict(self, X):
        output = []
        for document in X:
            extractions = self.keywords.extract_keywords(document)
            negations = [word for word in extractions if word.startswith("no ") or word.startswith("denies")]
            substances = [word for word in extractions if not word.startswith("no ")]

            if negations and not substances:
                output.append(0)  # Negation without substance use keywords -> No Substance Use
            elif substances and not negations:
                output.append(1)  # Substance use keywords without negations -> Substance Use
            else:
                output.append(0)  # Otherwise, assume no substance use
        return np.array(output)

    def predict_proba(self, X):
        return self.predict(X)

    def predict_log_proba(self, X):
        proba = self.predict_proba(X)
        return np.log(proba)


classifier_substance = SubstanceClassifier(kp_substance)

import pandas as pd
from sklearn.model_selection import train_test_split
df=notes_model.copy()
df.loc[df['substance_use']==-1, 'substance_use']=0
#X is refered to as the input data, y the labeled data
X = df.TEXT_mimic.to_list()
y = df['substance_use'].to_list()
#here we are breaking the data into two sets of approximately 50%, each with
X_train, X_test, y_train, y_test = train_test_split(X, y,stratify=y, test_size=0.5, shuffle=True, random_state=123)

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
from sklearn.metrics import classification_report
y_pred_test = classifier_substance.predict(X_test)
cm = confusion_matrix(y_test, y_pred_test)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=classifier_substance.classes_)
disp.plot()
print(classification_report(y_test, y_pred_test, target_names=classifier_substance.classes_))

"""# Word2Vec Embeddings

# Social Support
"""

!pip install --user --upgrade scikit-learn gensim nltk transformers
import nltk
nltk.download('punkt')

!python -m nltk.downloader stopwords

!python -m nltk.downloader wordnet
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from gensim.parsing.preprocessing import stem_text, preprocess_string, remove_stopwords, strip_multiple_whitespaces
def preprocess_data(notes):
    processed_notes = []
    for note in notes:
        filtered_tokens = [token for token in note if token not in stopwords]
        lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]
        # p=preprocess_string(note, filters=[lambda x: x.lower(), strip_multiple_whitespaces, remove_stopwords, stem_text])
        processed_notes.append(lemmatized_tokens)
    return processed_notes
df=notes_model.copy()
processed_notes = preprocess_data(df['TEXT_combined'])

import numpy as np
from gensim.models import Word2Vec

model = Word2Vec(sentences=processed_notes, vector_size=150, window=4, min_count=1, workers=6)

def note_to_vec(note):
    vecs = [model.wv[word] for word in note if word in model.wv]
    if len(vecs) > 0:
        return np.mean(vecs, axis=0)
    else:
        return np.zeros(model.vector_size)

note_vecs = np.array([note_to_vec(note) for note in processed_notes])

from sklearn.model_selection import train_test_split, cross_validate
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
labels=df['Social Support']
X_train, X_test, y_train, y_test = train_test_split(note_vecs, labels, test_size=0.2, random_state=42)
classifiers = [

    RandomForestClassifier(random_state=42),
    SVC(kernel='rbf', class_weight='balanced'),
    DecisionTreeClassifier(criterion='entropy', class_weight='balanced', random_state=42)
]

scoring = {
    'acc': 'accuracy',
    'precision': 'precision',
    'recall': 'recall',
    'f1': 'f1'
}

for clf in classifiers:
    scores = cross_validate(clf, X_train, y_train, scoring=scoring, cv=10, return_train_score=False)
    print("Classifier:", clf.__class__.__name__)
    print("Accuracy:", scores['test_acc'].mean())
    print("Precision:", scores['test_precision'].mean())
    print("Recall:", scores['test_recall'].mean())
    print("F1:", scores['test_f1'].mean())
    print()

"""# Occupation"""

!python -m nltk.downloader wordnet
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from gensim.parsing.preprocessing import stem_text, preprocess_string, remove_stopwords, strip_multiple_whitespaces
stopwords_list = stopwords.words('english')
lemmatizer = WordNetLemmatizer()

def preprocess_data(notes):
    processed_notes = []
    for note in notes:
        filtered_tokens = [token for token in note if token not in stopwords_list]
        lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]
        # p=preprocess_string(note, filters=[lambda x: x.lower(), strip_multiple_whitespaces, remove_stopwords, stem_text])
        processed_notes.append(lemmatized_tokens)
    return processed_notes
df['TEXT_mimic'] = df['TEXT_mimic'].replace(0, ' ')
processed_notes = preprocess_data(df['TEXT_mimic'])

import numpy as np
from gensim.models import Word2Vec

model = Word2Vec(sentences=processed_notes, vector_size=100, window=2, min_count=4, workers=4)

def note_to_vec(note):
    vecs = [model.wv[word] for word in note if word in model.wv]
    if len(vecs) > 0:
        return np.mean(vecs, axis=0)
    else:
        return np.zeros(model.vector_size)

note_vecs = np.array([note_to_vec(note) for note in processed_notes])
labels=df['Occupation']
X_train, X_test, y_train, y_test = train_test_split(note_vecs, labels, test_size=0.2, random_state=42)
classifiers = [

    RandomForestClassifier(random_state=42),
    SVC(kernel='linear', class_weight='balanced'),
    DecisionTreeClassifier(criterion='entropy', class_weight='balanced', random_state=42)
]

scoring = {
    'acc': 'accuracy',
    'precision': 'precision',
    'recall': 'recall',
    'f1': 'f1'
}

for clf in classifiers:
    scores = cross_validate(clf, X_train, y_train, scoring=scoring, cv=10, return_train_score=False)
    print("Classifier:", clf.__class__.__name__)
    print("Accuracy:", scores['test_acc'].mean())
    print("Precision:", scores['test_precision'].mean())
    print("Recall:", scores['test_recall'].mean())
    print("F1:", scores['test_f1'].mean())
    print()

"""# Substance Use"""

import numpy as np
from gensim.models import Word2Vec

model = Word2Vec(sentences=processed_notes, vector_size=100, window=2, min_count=1, workers=4)

def note_to_vec(note):
    vecs = [model.wv[word] for word in note if word in model.wv]
    if len(vecs) > 0:
        return np.mean(vecs, axis=0)
    else:
        return np.zeros(model.vector_size)

note_vecs = np.array([note_to_vec(note) for note in processed_notes])
labels=df['substance_use']
X_train, X_test, y_train, y_test = train_test_split(note_vecs, labels, test_size=0.2, random_state=42)
classifiers = [

    RandomForestClassifier(random_state=42),
    SVC(kernel='linear', class_weight='balanced'),
    DecisionTreeClassifier(criterion='entropy', class_weight='balanced', random_state=42)
]

scoring = {
    'acc': 'accuracy',
    'precision': 'precision',
    'recall': 'recall',
    'f1': 'f1'
}

for clf in classifiers:
    scores = cross_validate(clf, X_train, y_train, scoring=scoring, cv=10, return_train_score=False)
    print("Classifier:", clf.__class__.__name__)
    print("Accuracy:", scores['test_acc'].mean())
    print("Precision:", scores['test_precision'].mean())
    print("Recall:", scores['test_recall'].mean())
    print("F1:", scores['test_f1'].mean())
    print()

"""# Clinical BERT

# Social Support
"""

!pip install transformers
from transformers import BertTokenizer, BertModel
import torch
from sklearn.model_selection import train_test_split, cross_validate
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Load pre-trained BioBERT or ClinicalBERT tokenizer and model
from transformers import AutoTokenizer, AutoModel
tokenizer = AutoTokenizer.from_pretrained("emilyalsentzer/Bio_ClinicalBERT")

model = AutoModel.from_pretrained("emilyalsentzer/Bio_ClinicalBERT")
labels=df['Social Support']
def get_clinical_bert_embeddings(text, model, tokenizer, max_length=512):
    tokens = tokenizer.tokenize(text)
    if len(tokens) > max_length - 2:
        tokens = tokens[:max_length - 2]
    tokens = ['[CLS]'] + tokens + ['[SEP]']
    input_ids = tokenizer.convert_tokens_to_ids(tokens)
    input_ids = input_ids + [0] * (max_length - len(input_ids))
    input_ids = torch.tensor(input_ids).unsqueeze(0)
    outputs = model(input_ids)
    last_hidden_states = outputs[0]
    return last_hidden_states.squeeze().detach().mean(dim=0).numpy()

clinical_bert_embeddings = [get_clinical_bert_embeddings(' '.join(note), model, tokenizer) for note in processed_notes]

X_train, X_test, y_train, y_test = train_test_split(clinical_bert_embeddings, labels, test_size=0.2, random_state=42)

classifiers = [
    RandomForestClassifier(random_state=42),
    SVC(kernel='linear', class_weight='balanced'),
    DecisionTreeClassifier(criterion='entropy', class_weight='balanced', random_state=42)
]

scoring = {
    'acc': 'accuracy',
    'precision': 'precision',
    'recall': 'recall',
    'f1': 'f1'
}
for clf in classifiers:
    scores = cross_validate(clf, X_train, y_train, scoring=scoring, cv=5, return_train_score=False)
    print("Classifier:", clf.__class__.__name__)
    print("Accuracy:", scores['test_acc'].mean())
    print("Precision:", scores['test_precision'].mean())
    print("Recall:", scores['test_recall'].mean())
    print("F1:", scores['test_f1'].mean())
    print()

"""# Occupation"""

tokenizer = AutoTokenizer.from_pretrained("emilyalsentzer/Bio_ClinicalBERT")

model = AutoModel.from_pretrained("emilyalsentzer/Bio_ClinicalBERT")
labels=df['Occupation']
def get_clinical_bert_embeddings(text, model, tokenizer, max_length=512):
    tokens = tokenizer.tokenize(text)
    if len(tokens) > max_length - 2:
        tokens = tokens[:max_length - 2]
    tokens = ['[CLS]'] + tokens + ['[SEP]']
    input_ids = tokenizer.convert_tokens_to_ids(tokens)
    input_ids = input_ids + [0] * (max_length - len(input_ids))
    input_ids = torch.tensor(input_ids).unsqueeze(0)
    outputs = model(input_ids)
    last_hidden_states = outputs[0]
    return last_hidden_states.squeeze().detach().mean(dim=0).numpy()

clinical_bert_embeddings = [get_clinical_bert_embeddings(' '.join(note), model, tokenizer) for note in processed_notes]

X_train, X_test, y_train, y_test = train_test_split(clinical_bert_embeddings, labels, test_size=0.2, random_state=42)

classifiers = [
    RandomForestClassifier(random_state=42),
    SVC(kernel='rbf', class_weight='balanced'),
    DecisionTreeClassifier(criterion='entropy',class_weight='balanced', random_state=142)
]

scoring = {
    'acc': 'accuracy',
    'precision': 'precision',
    'recall': 'recall',
    'f1': 'f1'
}
for clf in classifiers:
    scores = cross_validate(clf, X_train, y_train, scoring=scoring, cv=5, return_train_score=False)
    print("Classifier:", clf.__class__.__name__)
    print("Accuracy:", scores['test_acc'].mean())
    print("Precision:", scores['test_precision'].mean())
    print("Recall:", scores['test_recall'].mean())
    print("F1:", scores['test_f1'].mean())
    print()

tokenizer = AutoTokenizer.from_pretrained("emilyalsentzer/Bio_ClinicalBERT")

model = AutoModel.from_pretrained("emilyalsentzer/Bio_ClinicalBERT")
labels=df['substance_use']
def get_clinical_bert_embeddings(text, model, tokenizer, max_length=512):
    tokens = tokenizer.tokenize(text)
    if len(tokens) > max_length - 2:
        tokens = tokens[:max_length - 2]
    tokens = ['[CLS]'] + tokens + ['[SEP]']
    input_ids = tokenizer.convert_tokens_to_ids(tokens)
    input_ids = input_ids + [0] * (max_length - len(input_ids))
    input_ids = torch.tensor(input_ids).unsqueeze(0)
    outputs = model(input_ids)
    last_hidden_states = outputs[0]
    return last_hidden_states.squeeze().detach().mean(dim=0).numpy()

clinical_bert_embeddings = [get_clinical_bert_embeddings(' '.join(note), model, tokenizer) for note in processed_notes]

X_train, X_test, y_train, y_test = train_test_split(clinical_bert_embeddings, labels, test_size=0.2, random_state=50)

classifiers = [
    RandomForestClassifier(random_state=42),
    SVC(kernel='poly', class_weight='balanced'),
    DecisionTreeClassifier(criterion='entropy',class_weight='balanced', random_state=42)
]

scoring = {
    'acc': 'accuracy',
    'precision': 'precision',
    'recall': 'recall',
    'f1': 'f1'
}
for clf in classifiers:
    scores = cross_validate(clf, X_train, y_train, scoring=scoring, cv=5, return_train_score=False)
    print("Classifier:", clf.__class__.__name__)
    print("Accuracy:", scores['test_acc'].mean())
    print("Precision:", scores['test_precision'].mean())
    print("Recall:", scores['test_recall'].mean())
    print("F1:", scores['test_f1'].mean())
    print()

